{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Cheat Sheets for AI, Neural Networks, Machine Learning, Deep Learning & Big  Data', metadata={'source': 'pdfs\\\\AI-Neural-Networks.pdf', 'page': 0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load Directory\n",
    "loader = PyPDFDirectoryLoader(\"./pdfs\")\n",
    "documents = loader.load()\n",
    "\n",
    "# use recursive character text splitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "final_documents = text_splitter.split_documents(documents)\n",
    "final_documents[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "429"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Generative_AI_Projects\\All_Genai_Project\\venv_lang_all_in_1\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Generative_AI_Projects\\All_Genai_Project\\venv_lang_all_in_1\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Asus\\.cache\\huggingface\\hub\\models--BAAI--bge-small-en-v1.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "huggingface_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name = \"BAAI/bge-small-en-v1.5\",\n",
    "    # sentence-transformers/all-MiniLM-16-v2\n",
    "    model_kwargs = {'device':'cpu'},\n",
    "    encode_kwargs = {'normalize_embeddings':True}\n",
    "        \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.06496848165988922,\n",
       " 0.009685522876679897,\n",
       " 0.0183071568608284,\n",
       " -0.022862335667014122,\n",
       " 0.06531897187232971,\n",
       " 0.05138126388192177,\n",
       " -0.046815723180770874,\n",
       " -0.011037111282348633,\n",
       " 0.047871559858322144,\n",
       " 0.0019477356690913439,\n",
       " 0.009998082183301449,\n",
       " -0.032154977321624756,\n",
       " 0.023536011576652527,\n",
       " 0.0015819245018064976,\n",
       " 0.03849292919039726,\n",
       " -0.0073563032783567905,\n",
       " -0.023745521903038025,\n",
       " -0.026339871808886528,\n",
       " 0.014911078847944736,\n",
       " -0.0015388522297143936,\n",
       " 0.012137327343225479,\n",
       " -0.026989618316292763,\n",
       " -0.011186380870640278,\n",
       " -0.022582747042179108,\n",
       " -0.01691521517932415,\n",
       " -0.01641758717596531,\n",
       " -0.014608647674322128,\n",
       " -0.11293288320302963,\n",
       " -0.06145765259861946,\n",
       " -0.19976578652858734,\n",
       " 0.013145742937922478,\n",
       " 0.009974789805710316,\n",
       " 0.04144194349646568,\n",
       " -0.018224624916911125,\n",
       " -0.010325283743441105,\n",
       " -0.009513627737760544,\n",
       " 0.024716386571526527,\n",
       " 0.035321373492479324,\n",
       " 0.03823104500770569,\n",
       " 0.03034066967666149,\n",
       " 0.03265480324625969,\n",
       " -0.03739696368575096,\n",
       " -0.004691861569881439,\n",
       " -0.052564360201358795,\n",
       " 0.08099338412284851,\n",
       " -0.046974413096904755,\n",
       " 0.006811013910919428,\n",
       " -0.006310302298516035,\n",
       " 0.044607456773519516,\n",
       " -0.014496972784399986,\n",
       " -0.04453054070472717,\n",
       " -0.03764324635267258,\n",
       " -0.04443708434700966,\n",
       " 0.024364782497286797,\n",
       " 0.005078883375972509,\n",
       " -0.02324543707072735,\n",
       " 0.09509710967540741,\n",
       " 0.09676756709814072,\n",
       " 0.054762523621320724,\n",
       " 0.033022910356521606,\n",
       " 0.03674039989709854,\n",
       " 0.018700724467635155,\n",
       " -0.09054041653871536,\n",
       " 0.06395553052425385,\n",
       " -0.016324449330568314,\n",
       " 0.05439642071723938,\n",
       " -0.019129620864987373,\n",
       " -0.042081624269485474,\n",
       " -0.018484346568584442,\n",
       " 0.06821348518133163,\n",
       " -0.024454515427350998,\n",
       " -0.009941508993506432,\n",
       " 0.0031381307635456324,\n",
       " 0.01265689730644226,\n",
       " 0.011901856400072575,\n",
       " 0.08482049405574799,\n",
       " 0.01692599058151245,\n",
       " -0.004258361645042896,\n",
       " -0.00961129181087017,\n",
       " -0.03939566761255264,\n",
       " -0.009879913181066513,\n",
       " 0.017750568687915802,\n",
       " -0.012277785688638687,\n",
       " 0.01155000925064087,\n",
       " -0.032829396426677704,\n",
       " -0.008541024290025234,\n",
       " -0.009163063019514084,\n",
       " 0.03797958046197891,\n",
       " 0.013799325563013554,\n",
       " -0.04415849968791008,\n",
       " 0.028590252622961998,\n",
       " 0.052878689020872116,\n",
       " -0.00045599357690662146,\n",
       " 0.023641087114810944,\n",
       " -0.07353230565786362,\n",
       " -0.004861493594944477,\n",
       " -0.020003560930490494,\n",
       " 0.03309537470340729,\n",
       " -0.004089679103344679,\n",
       " 0.35749852657318115,\n",
       " -0.03861314058303833,\n",
       " -0.040468476712703705,\n",
       " -0.018276061862707138,\n",
       " -0.012699753046035767,\n",
       " 0.008165303617715836,\n",
       " -0.02848341129720211,\n",
       " 0.003887805389240384,\n",
       " -0.042224060744047165,\n",
       " -0.0477386936545372,\n",
       " -0.021496793255209923,\n",
       " -0.019237086176872253,\n",
       " -0.01709868386387825,\n",
       " -0.01593492366373539,\n",
       " 0.002651836257427931,\n",
       " -0.019277451559901237,\n",
       " 0.0034341062419116497,\n",
       " 0.04536248743534088,\n",
       " -0.014447448775172234,\n",
       " 0.03859471157193184,\n",
       " 0.0023515343200415373,\n",
       " -0.045932836830616,\n",
       " 0.01989763416349888,\n",
       " 0.061584506183862686,\n",
       " 0.03598468378186226,\n",
       " -0.022795043885707855,\n",
       " -0.039597515016794205,\n",
       " 0.002651839517056942,\n",
       " 0.09877948462963104,\n",
       " -0.006043302360922098,\n",
       " 0.013723903335630894,\n",
       " 0.00355342635884881,\n",
       " 0.046834275126457214,\n",
       " -0.02083216980099678,\n",
       " 0.039246197789907455,\n",
       " 0.026039443910121918,\n",
       " 0.019168635830283165,\n",
       " 0.023416465148329735,\n",
       " -0.0047444491647183895,\n",
       " 0.011490704491734505,\n",
       " 0.020115455612540245,\n",
       " -0.06577000021934509,\n",
       " 0.11817333847284317,\n",
       " 0.10325009375810623,\n",
       " -0.05009407922625542,\n",
       " -0.0562065988779068,\n",
       " 0.01344285998493433,\n",
       " 0.0035297812428325415,\n",
       " 0.018943220376968384,\n",
       " -0.01644046977162361,\n",
       " -0.09360300749540329,\n",
       " 0.0015488667413592339,\n",
       " 0.026387587189674377,\n",
       " 0.011042226105928421,\n",
       " -0.09391927719116211,\n",
       " 0.04048848897218704,\n",
       " 0.034006476402282715,\n",
       " 0.04245581850409508,\n",
       " -0.03347880020737648,\n",
       " -0.09196628630161285,\n",
       " -0.0516006164252758,\n",
       " -0.045654479414224625,\n",
       " -0.013748785480856895,\n",
       " -0.06118781492114067,\n",
       " 0.10747276246547699,\n",
       " 0.00439362833276391,\n",
       " -0.13395828008651733,\n",
       " -0.006098938640207052,\n",
       " -0.02344144694507122,\n",
       " -0.015297053381800652,\n",
       " -0.0056881532073020935,\n",
       " 0.017655398696660995,\n",
       " -0.006331698037683964,\n",
       " -0.06745370477437973,\n",
       " 0.041619542986154556,\n",
       " 0.028693631291389465,\n",
       " 0.026798447594046593,\n",
       " -0.0762668177485466,\n",
       " -0.00882203783839941,\n",
       " -0.0014177431585267186,\n",
       " 0.025061091408133507,\n",
       " -0.027725448831915855,\n",
       " -0.07041126489639282,\n",
       " -0.005128169897943735,\n",
       " 0.007518551778048277,\n",
       " -0.030644705519080162,\n",
       " -0.0448111966252327,\n",
       " 0.007776421960443258,\n",
       " -0.0390324629843235,\n",
       " 0.004002792295068502,\n",
       " 0.013527491129934788,\n",
       " -0.00962953269481659,\n",
       " -0.011579413898289204,\n",
       " -0.08951298892498016,\n",
       " 0.05422519892454147,\n",
       " -0.03339015692472458,\n",
       " -0.06670501083135605,\n",
       " -0.014447773806750774,\n",
       " 0.013040397316217422,\n",
       " -0.03669776767492294,\n",
       " -0.057390909641981125,\n",
       " -0.03728324547410011,\n",
       " -0.043669842183589935,\n",
       " -0.00977347046136856,\n",
       " -0.009772391058504581,\n",
       " 0.0015004780143499374,\n",
       " -0.012694278731942177,\n",
       " 0.002437717281281948,\n",
       " -0.03579133003950119,\n",
       " 0.09642163664102554,\n",
       " 0.022155802696943283,\n",
       " -0.028496544808149338,\n",
       " -0.032012298703193665,\n",
       " -0.044258780777454376,\n",
       " 0.0018767707515507936,\n",
       " -0.02952924557030201,\n",
       " -0.0024514745455235243,\n",
       " 0.031291648745536804,\n",
       " 0.013312128372490406,\n",
       " -0.06512914597988129,\n",
       " 0.07166709750890732,\n",
       " 0.00726360222324729,\n",
       " -0.05111518129706383,\n",
       " -0.04093801602721214,\n",
       " -0.3016708791255951,\n",
       " -0.030936330556869507,\n",
       " -0.023252593353390694,\n",
       " 0.02143000066280365,\n",
       " -0.011148696765303612,\n",
       " -0.037178948521614075,\n",
       " 0.06043226644396782,\n",
       " 0.00245040375739336,\n",
       " 0.012938977219164371,\n",
       " 0.05856568366289139,\n",
       " 0.060079578310251236,\n",
       " 0.019437601789832115,\n",
       " -0.059706345200538635,\n",
       " -0.05172521620988846,\n",
       " -0.021059958264231682,\n",
       " -0.0007489896379411221,\n",
       " 0.027536025270819664,\n",
       " -0.006497655995190144,\n",
       " -0.04114021360874176,\n",
       " 0.0018423632718622684,\n",
       " -0.0014418578939512372,\n",
       " -0.002763360971584916,\n",
       " 0.0424208827316761,\n",
       " -0.04454399272799492,\n",
       " 0.005843776743859053,\n",
       " -0.018300049006938934,\n",
       " 0.1703183650970459,\n",
       " -0.027325373142957687,\n",
       " 0.03427872434258461,\n",
       " 0.011437894776463509,\n",
       " 0.007566665764898062,\n",
       " 0.030701223760843277,\n",
       " -0.0037746867164969444,\n",
       " -0.049103569239377975,\n",
       " 0.07556775212287903,\n",
       " -0.0007213115459308028,\n",
       " 0.02613932080566883,\n",
       " 0.0721752941608429,\n",
       " -0.05880708992481232,\n",
       " -0.022614995017647743,\n",
       " -0.064110666513443,\n",
       " 0.011916390620172024,\n",
       " -0.01596992462873459,\n",
       " -0.09226243942975998,\n",
       " -0.02461642026901245,\n",
       " -0.0016880026087164879,\n",
       " -0.03127744793891907,\n",
       " -0.045622777193784714,\n",
       " -0.02768503688275814,\n",
       " 0.018405446782708168,\n",
       " -0.026621606200933456,\n",
       " 0.0146663598716259,\n",
       " 0.018220415338873863,\n",
       " 0.0257128793746233,\n",
       " -0.024295056238770485,\n",
       " -0.0026662389282137156,\n",
       " -0.039139021188020706,\n",
       " 0.022947853431105614,\n",
       " -0.031042182818055153,\n",
       " 0.02395036444067955,\n",
       " 0.008375792764127254,\n",
       " -0.05725596472620964,\n",
       " -0.05817139521241188,\n",
       " -0.03373759984970093,\n",
       " 0.020286157727241516,\n",
       " -0.004437675699591637,\n",
       " 6.866556213935837e-05,\n",
       " 0.005351230036467314,\n",
       " 0.056136492639780045,\n",
       " 0.004176396876573563,\n",
       " 0.012710214592516422,\n",
       " 0.07331812381744385,\n",
       " -0.009854317642748356,\n",
       " -0.009806967340409756,\n",
       " 0.06691142171621323,\n",
       " 0.02199885994195938,\n",
       " 0.06929399073123932,\n",
       " -0.03637867793440819,\n",
       " -0.004006662871688604,\n",
       " 0.021712958812713623,\n",
       " 0.0471833273768425,\n",
       " 0.033741649240255356,\n",
       " 0.06849852204322815,\n",
       " -0.021613260731101036,\n",
       " 0.06557834148406982,\n",
       " 0.02120252326130867,\n",
       " 0.00509006530046463,\n",
       " -0.045012641698122025,\n",
       " 0.09821750223636627,\n",
       " 0.07370578497648239,\n",
       " 0.038015227764844894,\n",
       " -0.021373966708779335,\n",
       " -0.015155226923525333,\n",
       " -0.04157343506813049,\n",
       " 0.09616579860448837,\n",
       " 0.022546017542481422,\n",
       " -0.2155456691980362,\n",
       " 0.03474675118923187,\n",
       " 0.03806300088763237,\n",
       " 0.06686731427907944,\n",
       " -0.03310653567314148,\n",
       " -0.020712366327643394,\n",
       " 0.0029326491057872772,\n",
       " -0.09041675180196762,\n",
       " 0.02565702423453331,\n",
       " 0.011380554176867008,\n",
       " 0.029382333159446716,\n",
       " 0.015760231763124466,\n",
       " 0.029799941927194595,\n",
       " -0.06222730875015259,\n",
       " 0.02250507101416588,\n",
       " 0.011448182165622711,\n",
       " 0.08949661999940872,\n",
       " 0.0035845038946717978,\n",
       " 0.10607604682445526,\n",
       " -0.017296118661761284,\n",
       " 0.0020489615853875875,\n",
       " 0.0395645834505558,\n",
       " 0.18701505661010742,\n",
       " -0.017778243869543076,\n",
       " 0.01024265680462122,\n",
       " 0.06338398158550262,\n",
       " 0.023649105802178383,\n",
       " -0.016841528937220573,\n",
       " 0.04278680309653282,\n",
       " -0.028204331174492836,\n",
       " -0.025112781673669815,\n",
       " -0.011814609169960022,\n",
       " 0.052796076983213425,\n",
       " 0.016711758449673653,\n",
       " -0.02979082055389881,\n",
       " 0.08720758557319641,\n",
       " -0.0825757384300232,\n",
       " -0.002155164023861289,\n",
       " 0.053491443395614624,\n",
       " -0.00047193525824695826,\n",
       " 0.056359656155109406,\n",
       " -0.021874113008379936,\n",
       " -0.026803703978657722,\n",
       " 0.0490872785449028,\n",
       " 0.07233503460884094,\n",
       " 0.04277896508574486,\n",
       " -0.0430990606546402,\n",
       " -0.045122165232896805,\n",
       " -0.0034581448417156935,\n",
       " -0.019008342176675797,\n",
       " 0.014108463190495968,\n",
       " -0.016719721257686615,\n",
       " -0.04795602709054947,\n",
       " -0.03524186462163925,\n",
       " 0.034546297043561935,\n",
       " 0.05752310901880264,\n",
       " -0.024918507784605026,\n",
       " -0.002632350195199251,\n",
       " -0.002627841429784894,\n",
       " -0.007132549304515123,\n",
       " 0.028416093438863754,\n",
       " -0.03044840134680271,\n",
       " 0.06633163243532181,\n",
       " 0.041247542947530746,\n",
       " -0.05257419869303703]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "huggingface_embeddings.embed_query(final_documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.49684817e-02  9.68552288e-03  1.83071569e-02 -2.28623357e-02\n",
      "  6.53189719e-02  5.13812639e-02 -4.68157232e-02 -1.10371113e-02\n",
      "  4.78715599e-02  1.94773567e-03  9.99808218e-03 -3.21549773e-02\n",
      "  2.35360116e-02  1.58192450e-03  3.84929292e-02 -7.35630328e-03\n",
      " -2.37455219e-02 -2.63398718e-02  1.49110788e-02 -1.53885223e-03\n",
      "  1.21373273e-02 -2.69896183e-02 -1.11863809e-02 -2.25827470e-02\n",
      " -1.69152152e-02 -1.64175872e-02 -1.46086477e-02 -1.12932883e-01\n",
      " -6.14576526e-02 -1.99765787e-01  1.31457429e-02  9.97478981e-03\n",
      "  4.14419435e-02 -1.82246249e-02 -1.03252837e-02 -9.51362774e-03\n",
      "  2.47163866e-02  3.53213735e-02  3.82310450e-02  3.03406697e-02\n",
      "  3.26548032e-02 -3.73969637e-02 -4.69186157e-03 -5.25643602e-02\n",
      "  8.09933841e-02 -4.69744131e-02  6.81101391e-03 -6.31030230e-03\n",
      "  4.46074568e-02 -1.44969728e-02 -4.45305407e-02 -3.76432464e-02\n",
      " -4.44370843e-02  2.43647825e-02  5.07888338e-03 -2.32454371e-02\n",
      "  9.50971097e-02  9.67675671e-02  5.47625236e-02  3.30229104e-02\n",
      "  3.67403999e-02  1.87007245e-02 -9.05404165e-02  6.39555305e-02\n",
      " -1.63244493e-02  5.43964207e-02 -1.91296209e-02 -4.20816243e-02\n",
      " -1.84843466e-02  6.82134852e-02 -2.44545154e-02 -9.94150899e-03\n",
      "  3.13813076e-03  1.26568973e-02  1.19018564e-02  8.48204941e-02\n",
      "  1.69259906e-02 -4.25836165e-03 -9.61129181e-03 -3.93956676e-02\n",
      " -9.87991318e-03  1.77505687e-02 -1.22777857e-02  1.15500093e-02\n",
      " -3.28293964e-02 -8.54102429e-03 -9.16306302e-03  3.79795805e-02\n",
      "  1.37993256e-02 -4.41584997e-02  2.85902526e-02  5.28786890e-02\n",
      " -4.55993577e-04  2.36410871e-02 -7.35323057e-02 -4.86149359e-03\n",
      " -2.00035609e-02  3.30953747e-02 -4.08967910e-03  3.57498527e-01\n",
      " -3.86131406e-02 -4.04684767e-02 -1.82760619e-02 -1.26997530e-02\n",
      "  8.16530362e-03 -2.84834113e-02  3.88780539e-03 -4.22240607e-02\n",
      " -4.77386937e-02 -2.14967933e-02 -1.92370862e-02 -1.70986839e-02\n",
      " -1.59349237e-02  2.65183626e-03 -1.92774516e-02  3.43410624e-03\n",
      "  4.53624874e-02 -1.44474488e-02  3.85947116e-02  2.35153432e-03\n",
      " -4.59328368e-02  1.98976342e-02  6.15845062e-02  3.59846838e-02\n",
      " -2.27950439e-02 -3.95975150e-02  2.65183952e-03  9.87794846e-02\n",
      " -6.04330236e-03  1.37239033e-02  3.55342636e-03  4.68342751e-02\n",
      " -2.08321698e-02  3.92461978e-02  2.60394439e-02  1.91686358e-02\n",
      "  2.34164651e-02 -4.74444916e-03  1.14907045e-02  2.01154556e-02\n",
      " -6.57700002e-02  1.18173338e-01  1.03250094e-01 -5.00940792e-02\n",
      " -5.62065989e-02  1.34428600e-02  3.52978124e-03  1.89432204e-02\n",
      " -1.64404698e-02 -9.36030075e-02  1.54886674e-03  2.63875872e-02\n",
      "  1.10422261e-02 -9.39192772e-02  4.04884890e-02  3.40064764e-02\n",
      "  4.24558185e-02 -3.34788002e-02 -9.19662863e-02 -5.16006164e-02\n",
      " -4.56544794e-02 -1.37487855e-02 -6.11878149e-02  1.07472762e-01\n",
      "  4.39362833e-03 -1.33958280e-01 -6.09893864e-03 -2.34414469e-02\n",
      " -1.52970534e-02 -5.68815321e-03  1.76553987e-02 -6.33169804e-03\n",
      " -6.74537048e-02  4.16195430e-02  2.86936313e-02  2.67984476e-02\n",
      " -7.62668177e-02 -8.82203784e-03 -1.41774316e-03  2.50610914e-02\n",
      " -2.77254488e-02 -7.04112649e-02 -5.12816990e-03  7.51855178e-03\n",
      " -3.06447055e-02 -4.48111966e-02  7.77642196e-03 -3.90324630e-02\n",
      "  4.00279230e-03  1.35274911e-02 -9.62953269e-03 -1.15794139e-02\n",
      " -8.95129889e-02  5.42251989e-02 -3.33901569e-02 -6.67050108e-02\n",
      " -1.44477738e-02  1.30403973e-02 -3.66977677e-02 -5.73909096e-02\n",
      " -3.72832455e-02 -4.36698422e-02 -9.77347046e-03 -9.77239106e-03\n",
      "  1.50047801e-03 -1.26942787e-02  2.43771728e-03 -3.57913300e-02\n",
      "  9.64216366e-02  2.21558027e-02 -2.84965448e-02 -3.20122987e-02\n",
      " -4.42587808e-02  1.87677075e-03 -2.95292456e-02 -2.45147455e-03\n",
      "  3.12916487e-02  1.33121284e-02 -6.51291460e-02  7.16670975e-02\n",
      "  7.26360222e-03 -5.11151813e-02 -4.09380160e-02 -3.01670879e-01\n",
      " -3.09363306e-02 -2.32525934e-02  2.14300007e-02 -1.11486968e-02\n",
      " -3.71789485e-02  6.04322664e-02  2.45040376e-03  1.29389772e-02\n",
      "  5.85656837e-02  6.00795783e-02  1.94376018e-02 -5.97063452e-02\n",
      " -5.17252162e-02 -2.10599583e-02 -7.48989638e-04  2.75360253e-02\n",
      " -6.49765600e-03 -4.11402136e-02  1.84236327e-03 -1.44185789e-03\n",
      " -2.76336097e-03  4.24208827e-02 -4.45439927e-02  5.84377674e-03\n",
      " -1.83000490e-02  1.70318365e-01 -2.73253731e-02  3.42787243e-02\n",
      "  1.14378948e-02  7.56666576e-03  3.07012238e-02 -3.77468672e-03\n",
      " -4.91035692e-02  7.55677521e-02 -7.21311546e-04  2.61393208e-02\n",
      "  7.21752942e-02 -5.88070899e-02 -2.26149950e-02 -6.41106665e-02\n",
      "  1.19163906e-02 -1.59699246e-02 -9.22624394e-02 -2.46164203e-02\n",
      " -1.68800261e-03 -3.12774479e-02 -4.56227772e-02 -2.76850369e-02\n",
      "  1.84054468e-02 -2.66216062e-02  1.46663599e-02  1.82204153e-02\n",
      "  2.57128794e-02 -2.42950562e-02 -2.66623893e-03 -3.91390212e-02\n",
      "  2.29478534e-02 -3.10421828e-02  2.39503644e-02  8.37579276e-03\n",
      " -5.72559647e-02 -5.81713952e-02 -3.37375998e-02  2.02861577e-02\n",
      " -4.43767570e-03  6.86655621e-05  5.35123004e-03  5.61364926e-02\n",
      "  4.17639688e-03  1.27102146e-02  7.33181238e-02 -9.85431764e-03\n",
      " -9.80696734e-03  6.69114217e-02  2.19988599e-02  6.92939907e-02\n",
      " -3.63786779e-02 -4.00666287e-03  2.17129588e-02  4.71833274e-02\n",
      "  3.37416492e-02  6.84985220e-02 -2.16132607e-02  6.55783415e-02\n",
      "  2.12025233e-02  5.09006530e-03 -4.50126417e-02  9.82175022e-02\n",
      "  7.37057850e-02  3.80152278e-02 -2.13739667e-02 -1.51552269e-02\n",
      " -4.15734351e-02  9.61657986e-02  2.25460175e-02 -2.15545669e-01\n",
      "  3.47467512e-02  3.80630009e-02  6.68673143e-02 -3.31065357e-02\n",
      " -2.07123663e-02  2.93264911e-03 -9.04167518e-02  2.56570242e-02\n",
      "  1.13805542e-02  2.93823332e-02  1.57602318e-02  2.97999419e-02\n",
      " -6.22273088e-02  2.25050710e-02  1.14481822e-02  8.94966200e-02\n",
      "  3.58450389e-03  1.06076047e-01 -1.72961187e-02  2.04896159e-03\n",
      "  3.95645835e-02  1.87015057e-01 -1.77782439e-02  1.02426568e-02\n",
      "  6.33839816e-02  2.36491058e-02 -1.68415289e-02  4.27868031e-02\n",
      " -2.82043312e-02 -2.51127817e-02 -1.18146092e-02  5.27960770e-02\n",
      "  1.67117584e-02 -2.97908206e-02  8.72075856e-02 -8.25757384e-02\n",
      " -2.15516402e-03  5.34914434e-02 -4.71935258e-04  5.63596562e-02\n",
      " -2.18741130e-02 -2.68037040e-02  4.90872785e-02  7.23350346e-02\n",
      "  4.27789651e-02 -4.30990607e-02 -4.51221652e-02 -3.45814484e-03\n",
      " -1.90083422e-02  1.41084632e-02 -1.67197213e-02 -4.79560271e-02\n",
      " -3.52418646e-02  3.45462970e-02  5.75231090e-02 -2.49185078e-02\n",
      " -2.63235020e-03 -2.62784143e-03 -7.13254930e-03  2.84160934e-02\n",
      " -3.04484013e-02  6.63316324e-02  4.12475429e-02 -5.25741987e-02]\n",
      "(384,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.array(huggingface_embeddings.embed_query(final_documents[0].page_content)))\n",
    "print(np.array(huggingface_embeddings.embed_query(final_documents[0].page_content)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VectorStore Creation\n",
    "vectorstore=FAISS.from_documents(final_documents[:120],huggingface_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cheat Sheets for AI, Neural Networks, Machine Learning, Deep Learning & Big  Data\n"
     ]
    }
   ],
   "source": [
    "## Query using Similarity Search\n",
    "query=\"give me classification algorithm?\"\n",
    "relevant_docments=vectorstore.similarity_search(query)\n",
    "\n",
    "print(relevant_docments[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags=['FAISS', 'HuggingFaceBgeEmbeddings'] vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001F6E7245F60> search_kwargs={'k': 3}\n"
     ]
    }
   ],
   "source": [
    "retriever=vectorstore.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":3})\n",
    "print(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN']= os.getenv('HUGGINGFACE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\Generative_AI_Projects\\All_Genai_Project\\venv_lang_all_in_1\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Generative_AI_Projects\\All_Genai_Project\\venv_lang_all_in_1\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 503 Server Error: Service Unavailable for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32md:\\Generative_AI_Projects\\All_Genai_Project\\venv_lang_all_in_1\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:267\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 267\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32md:\\Generative_AI_Projects\\All_Genai_Project\\venv_lang_all_in_1\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:371\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 371\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 503 Server Error: Service Unavailable for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1 (Request ID: k6S5XrnXFGI95MzW-jQZu)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 9\u001b[0m\n\u001b[0;32m      3\u001b[0m hf\u001b[38;5;241m=\u001b[39mHuggingFaceHub(\n\u001b[0;32m      4\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     model_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m500\u001b[39m}\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow can I generate primary key values for my table?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mhf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Generative_AI_Projects\\All_Genai_Project\\venv_lang_all_in_1\\lib\\site-packages\\langchain_core\\language_models\\llms.py:276\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    274\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 276\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    277\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    278\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    279\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    280\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    281\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    282\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    283\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    284\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    285\u001b[0m         )\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    288\u001b[0m     )\n",
      "File \u001b[1;32md:\\Generative_AI_Projects\\All_Genai_Project\\venv_lang_all_in_1\\lib\\site-packages\\langchain_core\\language_models\\llms.py:633\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    627\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    631\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    632\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Generative_AI_Projects\\All_Genai_Project\\venv_lang_all_in_1\\lib\\site-packages\\langchain_core\\language_models\\llms.py:803\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    789\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    790\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    791\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    801\u001b[0m         )\n\u001b[0;32m    802\u001b[0m     ]\n\u001b[1;32m--> 803\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    804\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    805\u001b[0m     )\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Generative_AI_Projects\\All_Genai_Project\\venv_lang_all_in_1\\lib\\site-packages\\langchain_core\\language_models\\llms.py:670\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    669\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    671\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32md:\\Generative_AI_Projects\\All_Genai_Project\\venv_lang_all_in_1\\lib\\site-packages\\langchain_core\\language_models\\llms.py:657\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    649\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    654\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    656\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 657\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    658\u001b[0m                 prompts,\n\u001b[0;32m    659\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    660\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    661\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    662\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    663\u001b[0m             )\n\u001b[0;32m    664\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    665\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    666\u001b[0m         )\n\u001b[0;32m    667\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    668\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32md:\\Generative_AI_Projects\\All_Genai_Project\\venv_lang_all_in_1\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1317\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1314\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1316\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1317\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1318\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1319\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1320\u001b[0m     )\n\u001b[0;32m   1321\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32md:\\Generative_AI_Projects\\All_Genai_Project\\venv_lang_all_in_1\\lib\\site-packages\\langchain_community\\llms\\huggingface_hub.py:135\u001b[0m, in \u001b[0;36mHuggingFaceHub._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m _model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    133\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_model_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m--> 135\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m response \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response:\n",
      "File \u001b[1;32md:\\Generative_AI_Projects\\All_Genai_Project\\venv_lang_all_in_1\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:285\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# ...or wait 1s and retry\u001b[39;00m\n\u001b[0;32m    284\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting for model to be loaded on the server: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 285\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    287\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout \u001b[38;5;241m-\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0), \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "hf=HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "    model_kwargs={\"temperature\":0.1,\"max_length\":500}\n",
    "\n",
    ")\n",
    "query=\"How can I generate primary key values for my table?\"\n",
    "hf.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN']= os.getenv('HUGGINGFACE_API_KEY')\n",
    "\n",
    "huggingface_embeddings.embed_query(final_documents[0].page_content)\n",
    "\n",
    "# print(np.array(huggingface_embeddings.embed_query(final_documents[2].page_content)))\n",
    "\n",
    "## VectorStore Creation\n",
    "vectorstore=FAISS.from_documents(final_documents[:200],huggingface_embeddings)\n",
    "## Query using Similarity Search\n",
    "\n",
    "# query=\"How can I generate primary key values for my table?\"\n",
    "# relevant_documents=vectorstore.similarity_search(query)\n",
    "# print(relevant_documents[0].page_content)\n",
    "# retriever=vectorstore.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":3})\n",
    "# print(retriever)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "# hf=HuggingFaceHub(\n",
    "#     repo_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "#     model_kwargs={\"temperature\":0.1,\"max_length\":500}\n",
    "\n",
    "# )\n",
    "# query=\"How can I generate primary key values for my table?\"\n",
    "# hf.invoke(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11. To reverse\n",
      "12. How can I eliminate duplicate values in a table?\n",
      "Choose one of the following queries to identify or remove duplicate rows from a table leaving one\n",
      "record:\n",
      "Method 1:\n",
      "DELETE FROM table_name A WHERE ROWID > (SELECT min (rowid) FROM table_name B\n",
      "tags=['FAISS', 'HuggingFaceBgeEmbeddings'] vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001F6E9257EB0> search_kwargs={'k': 3}\n"
     ]
    }
   ],
   "source": [
    "# Query using Similarity Search\n",
    "\n",
    "query=\"How can I eliminate duplicate values in a table\"\n",
    "relevant_documents=vectorstore.similarity_search(query)\n",
    "print(relevant_documents[0].page_content)\n",
    "retriever=vectorstore.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":3})\n",
    "print(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1 (Request ID: -wLdQVm6PWXu1WO1nrZe5)\n\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\Generative_AI_Projects\\All_Genai_Project\\venv_lang_all_in_1\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Generative_AI_Projects\\All_Genai_Project\\venv_lang_all_in_1\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 9\u001b[0m\n\u001b[0;32m      3\u001b[0m hf\u001b[38;5;241m=\u001b[39mHuggingFaceHub(\n\u001b[0;32m      4\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     model_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m500\u001b[39m}\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow can I generate primary key values for my table?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mhf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Generative_AI_Projects\\All_Genai_Project\\venv_lang_all_in_1\\lib\\site-packages\\langchain_core\\language_models\\llms.py:276\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    274\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 276\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    277\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    278\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    279\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    280\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    281\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    282\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    283\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    284\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    285\u001b[0m         )\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    288\u001b[0m     )\n",
      "File \u001b[1;32md:\\Generative_AI_Projects\\All_Genai_Project\\venv_lang_all_in_1\\lib\\site-packages\\langchain_core\\language_models\\llms.py:633\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    627\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    631\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    632\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Generative_AI_Projects\\All_Genai_Project\\venv_lang_all_in_1\\lib\\site-packages\\langchain_core\\language_models\\llms.py:803\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    789\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    790\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    791\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    801\u001b[0m         )\n\u001b[0;32m    802\u001b[0m     ]\n\u001b[1;32m--> 803\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    804\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    805\u001b[0m     )\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Generative_AI_Projects\\All_Genai_Project\\venv_lang_all_in_1\\lib\\site-packages\\langchain_core\\language_models\\llms.py:670\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    669\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    671\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32md:\\Generative_AI_Projects\\All_Genai_Project\\venv_lang_all_in_1\\lib\\site-packages\\langchain_core\\language_models\\llms.py:657\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    649\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    654\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    656\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 657\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    658\u001b[0m                 prompts,\n\u001b[0;32m    659\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    660\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    661\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    662\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    663\u001b[0m             )\n\u001b[0;32m    664\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    665\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    666\u001b[0m         )\n\u001b[0;32m    667\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    668\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32md:\\Generative_AI_Projects\\All_Genai_Project\\venv_lang_all_in_1\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1317\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1314\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1316\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1317\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1318\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1319\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1320\u001b[0m     )\n\u001b[0;32m   1321\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32md:\\Generative_AI_Projects\\All_Genai_Project\\venv_lang_all_in_1\\lib\\site-packages\\langchain_community\\llms\\huggingface_hub.py:135\u001b[0m, in \u001b[0;36mHuggingFaceHub._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m _model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    133\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_model_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m--> 135\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m response \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response:\n",
      "File \u001b[1;32md:\\Generative_AI_Projects\\All_Genai_Project\\venv_lang_all_in_1\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:267\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 267\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32md:\\Generative_AI_Projects\\All_Genai_Project\\venv_lang_all_in_1\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:371\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 371\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1 (Request ID: -wLdQVm6PWXu1WO1nrZe5)\n\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "hf=HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "    model_kwargs={\"temperature\":0.1,\"max_length\":500}\n",
    "\n",
    ")\n",
    "query=\"How can I generate primary key values for my table?\"\n",
    "hf.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face models can be run locally through the HuggingFacePipeline class.\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"temperature\": 0, \"max_new_tokens\": 300}\n",
    ")\n",
    "\n",
    "llm = hf \n",
    "llm.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\"\n",
    "Use the following piece of context to answer the question asked.\n",
    "Please try to provide the answer only based on the context\n",
    "\n",
    "{context}\n",
    "Question:{question}\n",
    "\n",
    "Helpful Answers:\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(template=prompt_template,input_variables=[\"context\",\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievalQA=RetrievalQA.from_chain_type(\n",
    "    llm=hf,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\":prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"DIFFERENCES IN THE\n",
    "UNINSURED RATE BY STATE\n",
    "IN 2022\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the QA chain with our query.\n",
    "result = retrievalQA.invoke({\"query\": query})\n",
    "print(result['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
